{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b8e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from botorch.models import FixedNoiseGP\n",
    "from botorch.models.model_list_gp_regression import ModelListGP\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from gpytorch.mlls.sum_marginal_log_likelihood import SumMarginalLogLikelihood\n",
    "from botorch.acquisition.multi_objective.monte_carlo import qExpectedHypervolumeImprovement\n",
    "from botorch.utils.transforms import normalize, unnormalize\n",
    "from botorch.utils.multi_objective.hypervolume import Hypervolume\n",
    "from botorch.sampling.normal import SobolQMCNormalSampler\n",
    "from botorch.utils.multi_objective.box_decompositions.non_dominated import NondominatedPartitioning\n",
    "from botorch.utils.multi_objective.pareto import is_non_dominated\n",
    "from botorch.utils.testing import BotorchTestCase\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from botorch.utils.multi_objective import is_non_dominated\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "PLOTS_DIR = \"plots\"\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "print(f\"âœ… Created directory '{PLOTS_DIR}/' for saving plots.\")\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "def plot_pareto_front(train_Y, iteration_counter, initial_size, new_points=None, save_path=None):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 7))\n",
    "    \n",
    "    evaluated_data = -train_Y.cpu().numpy()\n",
    "    from botorch.utils.multi_objective.pareto import is_non_dominated\n",
    "    is_pareto = is_non_dominated(train_Y).cpu().numpy()\n",
    "    \n",
    "    ax.scatter(evaluated_data[:, 0], evaluated_data[:, 1], c='silver', alpha=0.4, s=30, label='Evaluated')\n",
    "    \n",
    "    ax.scatter(evaluated_data[:initial_size, 0], evaluated_data[:initial_size, 1], \n",
    "               c='white', edgecolors='#1f77b4', linewidth=1.5, s=60, label='Initial Set')\n",
    "    \n",
    "    pareto_points = evaluated_data[is_pareto]\n",
    "    sort_idx = np.argsort(pareto_points[:, 0])\n",
    "    ax.step(pareto_points[sort_idx, 0], pareto_points[sort_idx, 1], where='post', c='red', alpha=0.3, linestyle='--')\n",
    "    ax.scatter(pareto_points[:, 0], pareto_points[:, 1], c='#d62728', s=120, marker='*', edgecolors='black', linewidth=0.5, label='Pareto Front', zorder=10)\n",
    "    \n",
    "    if new_points is not None:\n",
    "        new_data = -new_points.cpu().numpy()\n",
    "        ax.scatter(new_data[:, 0], new_data[:, 1], c='#2ca02c', s=100, marker='P', edgecolors='black', label='New Candidates', zorder=11)\n",
    "    \n",
    "    ax.set_xlabel(r'Reaction Free Energy $\\Delta G$ (kcal/mol)', fontsize=13)\n",
    "    ax.set_ylabel(r'Activation Free Energy $\\Delta G^{\\ddagger}$ (kcal/mol)', fontsize=13)\n",
    "    ax.set_title(f\"Pareto Optimization: Iteration {iteration_counter - 1}\", fontsize=15, fontweight='bold')\n",
    "    ax.legend(frameon=True, loc='upper right', fontsize=10)\n",
    "    ax.grid(True, linestyle=':', alpha=0.6)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_acquisition_function_large_scale(ehvi_values, next_candidate_indices, save_path=None):\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    x_pos = np.arange(len(ehvi_values))\n",
    "    ax.fill_between(x_pos, ehvi_values, color='skyblue', alpha=0.4)\n",
    "    ax.plot(x_pos, ehvi_values, color='#1f77b4', linewidth=0.8, alpha=0.7)\n",
    "    \n",
    "    selected_values = ehvi_values[next_candidate_indices]\n",
    "    ax.vlines(next_candidate_indices, 0, selected_values, color='red', linestyle='-', alpha=0.6)\n",
    "    ax.scatter(next_candidate_indices, selected_values, color='red', s=40, zorder=5, label='Selected for Batch')\n",
    "    \n",
    "    ax.set_xlabel(\"Candidate Index\", fontsize=12)\n",
    "    ax.set_ylabel(r\"$\\alpha_{EHVI}(\\mathbf{x})$\", fontsize=14)\n",
    "    ax.set_title(\"Expected Hypervolume Improvement Distribution\", fontsize=14)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "    ax.legend()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_feature_space(X_pool, train_X, initial_size, new_X=None, save_path=None):\n",
    "    if X_pool.shape[1] < 2: return\n",
    "        \n",
    "    pca = PCA(n_components=2)\n",
    "    X_pool_2d = pca.fit_transform(X_pool.cpu().numpy())\n",
    "    train_X_2d = pca.transform(train_X.cpu().numpy())\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 7))\n",
    "\n",
    "    ax.scatter(X_pool_2d[:, 0], X_pool_2d[:, 1], c='whitesmoke', s=20, edgecolors='gainsboro', label='Chemical Space')\n",
    "    \n",
    "    ax.scatter(train_X_2d[:initial_size, 0], train_X_2d[:initial_size, 1], \n",
    "               c='#1f77b4', s=50, alpha=0.7, label='Initial Points')\n",
    "    \n",
    "    end_idx = len(train_X_2d) - (len(new_X) if new_X is not None else 0)\n",
    "    ax.scatter(train_X_2d[initial_size:end_idx, 0], train_X_2d[initial_size:end_idx, 1], \n",
    "               c='#ff7f0e', s=50, alpha=0.7, label='BO Iterations')\n",
    "    \n",
    "    if new_X is not None:\n",
    "        new_X_2d = pca.transform(new_X.cpu().numpy())\n",
    "        ax.scatter(new_X_2d[:, 0], new_X_2d[:, 1], c='#2ca02c', s=120, marker='P', edgecolors='black', label='Next Batch')\n",
    "    \n",
    "    ax.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.1%} var.)\", fontsize=12)\n",
    "    ax.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.1%} var.)\", fontsize=12)\n",
    "    ax.set_title(\"Exploration in Latent Feature Space\", fontsize=14)\n",
    "    ax.legend(loc='best', frameon=True, shadow=True)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_gp_regression(model, train_X, train_Y, standardizer, bounds, objective_names, save_path=None):\n",
    "    model.eval()\n",
    "    latex_names = [r\"$\\Delta G$\", r\"$\\Delta G^{\\ddagger}$\"]\n",
    "    \n",
    "    normalized_train_X = normalize(train_X, bounds)\n",
    "    with torch.no_grad():\n",
    "        posterior = model.posterior(normalized_train_X)\n",
    "    \n",
    "    mean_pred_unstandardized = standardizer.untransform(posterior.mean)[0]\n",
    "    y_pred_orig = -mean_pred_unstandardized.cpu().numpy()\n",
    "    y_true_orig = -train_Y.cpu().numpy()\n",
    "    \n",
    "    M = train_Y.shape[-1]\n",
    "    fig, axes = plt.subplots(1, M, figsize=(12, 5))\n",
    "    if M == 1: axes = [axes]\n",
    "    \n",
    "    for i in range(M):\n",
    "        ax = axes[i]\n",
    "        y_t, y_p = y_true_orig[:, i], y_pred_orig[:, i]\n",
    "        \n",
    "        lims = [min(y_t.min(), y_p.min())-1, max(y_t.max(), y_p.max())+1]\n",
    "        ax.plot(lims, lims, 'k--', alpha=0.5, zorder=0)\n",
    "\n",
    "        rmse = np.sqrt(np.mean((y_t - y_p)**2))\n",
    "        ax.scatter(y_t, y_p, c='#17becf', edgecolors='navy', alpha=0.6, s=40, label=f'RMSE: {rmse:.2f}')\n",
    "        \n",
    "        ax.set_xlabel(f\"True {latex_names[i]} (kcal/mol)\", fontsize=12)\n",
    "        ax.set_ylabel(f\"GP Predicted {latex_names[i]}\", fontsize=12)\n",
    "        ax.set_title(f\"Model Fidelity: {latex_names[i]}\", fontsize=14)\n",
    "        ax.legend()\n",
    "        ax.set_aspect('equal')\n",
    "        ax.grid(True, linestyle=':', alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_hypervolume_evolution(hypervolume_history, initial_hv, save_path=None):\n",
    "    fig, ax = plt.subplots(figsize=(9, 6))\n",
    "    \n",
    "    all_hvs = [initial_hv] + hypervolume_history\n",
    "    x_pos = np.arange(len(all_hvs))\n",
    "    \n",
    "    ax.fill_between(x_pos, all_hvs, initial_hv, color='lavender', alpha=0.5)\n",
    "    ax.plot(x_pos, all_hvs, marker='o', markersize=8, linewidth=2, color='#58508d', markerfacecolor='#ffa600', label='Observed HV')\n",
    "    \n",
    "    ax.set_xlabel(\"Optimization Step (Batch Index)\", fontsize=12)\n",
    "    ax.set_ylabel(\"Hypervolume (HV)\", fontsize=12)\n",
    "    ax.set_title(\"Optimization Convergence: Hypervolume Progression\", fontsize=14)\n",
    "    \n",
    "    ax.set_xticks(x_pos)\n",
    "    labels = ['Init'] + [str(i) for i in range(1, len(all_hvs))]\n",
    "    ax.set_xticklabels(labels)\n",
    "    \n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    ax.legend(loc='lower right')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "SEED = 12345 \n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    \n",
    "# BotorchTestCase.seed(SEED)\n",
    "print(f\"âœ… Global random seed set to: {SEED}\")\n",
    "\n",
    "tkwargs = {\n",
    "    \"dtype\": torch.double,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}\n",
    "\n",
    "print(f\"Using device: {tkwargs['device']}\")\n",
    "\n",
    "try:\n",
    "    df_pool = pd.read_csv(\"../../descriptors_cleaned.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Please ensure your data file is named 'descriptors_cleaned.csv' and is located in the same folder as the script.\")\n",
    "    \n",
    "CANDIDATE_NAMES_POOL = df_pool.iloc[:, 0].tolist()\n",
    "X_CANDIDATES_POOL = torch.tensor(df_pool.iloc[:, 1:].values, **tkwargs)\n",
    "FEATURE_COLUMNS = df_pool.columns[1:]\n",
    "\n",
    "D = X_CANDIDATES_POOL.shape[1]\n",
    "M = 2\n",
    "BOUNDS = torch.stack([X_CANDIDATES_POOL.min(dim=0).values, X_CANDIDATES_POOL.max(dim=0).values])\n",
    "print(f\"Successfully loaded {len(CANDIDATE_NAMES_POOL)} candidate molecules with {D} descriptors each.\")\n",
    "print(\"\\n--- Performing Data Cleaning ---\")\n",
    "\n",
    "features_df = df_pool.iloc[:, 1:]\n",
    "nan_columns = features_df.columns[features_df.isnull().any()].tolist()\n",
    "\n",
    "constant_columns = features_df.columns[features_df.nunique() == 1].tolist()\n",
    "\n",
    "columns_to_drop = list(set(nan_columns + constant_columns))\n",
    "if columns_to_drop:\n",
    "    print(\"\\nâš ï¸ Found the following problematic feature columns to be removed:\")\n",
    "    if nan_columns:\n",
    "        print(f\"  - {len(nan_columns)} columns contain NaN values: {nan_columns}\")\n",
    "    if constant_columns:\n",
    "        print(f\"  - {len(constant_columns)} columns are constant: {constant_columns}\")\n",
    "\n",
    "    df_pool = df_pool.drop(columns=columns_to_drop)\n",
    "\n",
    "    print(f\"\\nâœ… Removed all problematic features. Remaining feature count: {df_pool.shape[1] - 1}\")\n",
    "else:\n",
    "    print(\"\\nâœ… Dataset is clean with no constant or NaN-containing feature columns.\")\n",
    "\n",
    "rows_to_remove_mask = df_pool['name'].str.contains('frame_6')\n",
    "\n",
    "num_to_remove = rows_to_remove_mask.sum()\n",
    "original_rows = len(df_pool)\n",
    "print(f\"Initial DataFrame has {original_rows} rows.\")\n",
    "print(f\"Identified {num_to_remove} rows with 'name' containing 'frame_6' to be removed.\")\n",
    "\n",
    "df_pool_cleaned = df_pool[~rows_to_remove_mask]\n",
    "final_rows = len(df_pool_cleaned)\n",
    "\n",
    "print(f\"Operation completed. The new DataFrame has {final_rows} rows.\")\n",
    "print(f\"Successfully removed {original_rows - final_rows} rows.\")\n",
    "df_pool = df_pool_cleaned\n",
    "\n",
    "X_CANDIDATES_POOL = torch.tensor(df_pool.iloc[:, 1:].values, **tkwargs)\n",
    "FEATURE_COLUMNS = df_pool.columns[1:]\n",
    "D = X_CANDIDATES_POOL.shape[1]\n",
    "BOUNDS = torch.stack([X_CANDIDATES_POOL.min(dim=0).values, X_CANDIDATES_POOL.max(dim=0).values])\n",
    "print(f\"Data cleaning completed. Updated feature dimension: D={D}\")\n",
    "\n",
    "N_INITIAL = 5\n",
    "INITIAL_CSV_PATH = \"initial_candidates.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dfbb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_initial_results = pd.read_csv(INITIAL_CSV_PATH)\n",
    "df_initial = pd.read_csv(INITIAL_CSV_PATH)\n",
    "INITIAL_TRAIN_SIZE = len(df_initial)\n",
    "\n",
    "\n",
    "if df_initial_results[['dG', 'dG_ts']].isnull().values.any():\n",
    "    print(\"ðŸ›‘ Error: 'initial_candidates.csv' still contains unfilled data. Please complete the calculations and fill in the data before running this cell again.\")\n",
    "else:\n",
    "    train_Y = torch.tensor(-df_initial_results[['dG', 'dG_ts']].values, **tkwargs)\n",
    "\n",
    "    train_X_list = []\n",
    "    for name in df_initial_results['name']:\n",
    "        idx = CANDIDATE_NAMES_POOL.index(name)\n",
    "        train_X_list.append(X_CANDIDATES_POOL[idx])\n",
    "    train_X = torch.stack(train_X_list)\n",
    "\n",
    "    evaluated_names = set(df_initial_results['name'].tolist())\n",
    "\n",
    "    iteration_counter = 1\n",
    "\n",
    "    print(\"\\nâœ… Initial data loaded successfully!\")\n",
    "    print(f\"Current training set size: {len(train_X)}\")\n",
    "\n",
    "ref_point_hv = train_Y.min(dim=0).values - 0.2\n",
    "hv_calculator = Hypervolume(ref_point=ref_point_hv)\n",
    "initial_hypervolume = hv_calculator.compute(train_Y)\n",
    "hypervolume_history = [] \n",
    "print(f\"\\nâœ… Initial hypervolume calculation completed: {initial_hypervolume:.4f}\")\n",
    "print(f\"   (using reference point: {ref_point_hv.cpu().numpy()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3277d0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "\n",
    "print(f\"\\n--- Iteration {iteration_counter}: Suggesting next batch of {BATCH_SIZE} candidate points ---\")\n",
    "\n",
    "standardizer = Standardize(m=M)\n",
    "train_Y_standardized, _ = standardizer(train_Y)\n",
    "\n",
    "models = []\n",
    "for j in range(M):\n",
    "    train_y_j_standardized_col = train_Y_standardized[:, j].unsqueeze(-1)\n",
    "    models.append(\n",
    "        FixedNoiseGP(\n",
    "            train_X=normalize(train_X, BOUNDS),\n",
    "            train_Y=train_y_j_standardized_col,\n",
    "            train_Yvar=torch.full_like(train_y_j_standardized_col, 1e-6)\n",
    "\n",
    "        )\n",
    "    )\n",
    "model = ModelListGP(*models)\n",
    "mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "fit_gpytorch_mll(mll)\n",
    "print(\"\\n--- Plotting current GP model regression performance ---\")\n",
    "plot_gp_regression(\n",
    "    model=model,\n",
    "    train_X=train_X,\n",
    "    train_Y=train_Y,\n",
    "    standardizer=standardizer,\n",
    "    bounds=BOUNDS,\n",
    "    objective_names=['dG', 'dG_ts'],\n",
    "    save_path=f\"{PLOTS_DIR}/gp_regression_iter_{iteration_counter}.png\"\n",
    ")\n",
    "\n",
    "ref_point = train_Y.min(dim=0).values - 0.2\n",
    "acq_func = qExpectedHypervolumeImprovement(\n",
    "    model=model,\n",
    "    ref_point=ref_point,\n",
    "    partitioning=NondominatedPartitioning(ref_point=ref_point, Y=train_Y_standardized),\n",
    "    sampler=SobolQMCNormalSampler(sample_shape=torch.Size([128])),\n",
    ")\n",
    "\n",
    "unseen_mask = ~df_pool['name'].isin(evaluated_names)\n",
    "unseen_df = df_pool[unseen_mask]\n",
    "unseen_indices = unseen_df.index.tolist()\n",
    "unseen_X_candidates = X_CANDIDATES_POOL[unseen_indices]\n",
    "\n",
    "if len(unseen_df) < BATCH_SIZE:\n",
    "    print(\"\\nâš ï¸ Warning: The number of remaining candidate molecules is less than the batch size.\")\n",
    "    current_batch_size = len(unseen_df)\n",
    "else:\n",
    "    current_batch_size = BATCH_SIZE\n",
    "print(f\"\\n--- Iteration {iteration_counter}: Evaluating EHVI for {len(unseen_df)} remaining candidate molecules ---\")\n",
    "\n",
    "normalized_unseen_X = normalize(unseen_X_candidates, BOUNDS)\n",
    "ACQ_EVAL_BATCH_SIZE = 1024 \n",
    "ehvi_values_list = []\n",
    "candidate_batches = torch.split(normalized_unseen_X, ACQ_EVAL_BATCH_SIZE)\n",
    "\n",
    "print(f\"\\n--- Splitting into {len(candidate_batches)} batches of size {ACQ_EVAL_BATCH_SIZE} for EHVI evaluation to save memory ---\")\n",
    "with torch.no_grad():\n",
    "    for i, X_batch in enumerate(candidate_batches):\n",
    "        print(f\"  - Now processing batch {i+1}/{len(candidate_batches)}...\")\n",
    "        ehvi_values_list.append(acq_func(X_batch.unsqueeze(1)))\n",
    "        \n",
    "ehvi_values = torch.cat(ehvi_values_list)\n",
    "\n",
    "_, top_local_indices = torch.topk(ehvi_values, k=current_batch_size) \n",
    "\n",
    "plot_acquisition_function_large_scale(\n",
    "    ehvi_values=ehvi_values.cpu().numpy(),\n",
    "    next_candidate_indices=top_local_indices.cpu().numpy(),\n",
    "    save_path=f\"{PLOTS_DIR}/ehvi_dist_iter_{iteration_counter}.png\"\n",
    ")\n",
    "top_original_indices = [unseen_indices[i] for i in top_local_indices]\n",
    "next_candidates_df = df_pool.iloc[top_original_indices].copy()\n",
    "\n",
    "next_candidates_df['dG'] = np.nan\n",
    "next_candidates_df['dG_ts'] = np.nan\n",
    "NEXT_CSV_PATH = f\"iteration_{iteration_counter}_candidates_batch.csv\"\n",
    "next_candidates_df.to_csv(NEXT_CSV_PATH, index=False)\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Successfully suggested the next batch of {current_batch_size} candidate molecules.\")\n",
    "print(f\"Task saved to file: '{NEXT_CSV_PATH}'\")\n",
    "print(\"Suggested molecule list:\")\n",
    "print(next_candidates_df[['name']])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c4c9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(NEXT_CSV_PATH):\n",
    "    print(f\"ðŸ›‘ File '{NEXT_CSV_PATH}' does not exist. Please run Cell 4 to generate the recommendation file first.\")\n",
    "else:\n",
    "    df_new_batch = pd.read_csv(NEXT_CSV_PATH)\n",
    "\n",
    "    df_success = df_new_batch.dropna(subset=['dG', 'dG_ts']).copy()\n",
    "\n",
    "    all_names_in_batch = df_new_batch['name'].tolist()\n",
    "\n",
    "    evaluated_names.update(all_names_in_batch)\n",
    "    \n",
    "    if df_success.empty:\n",
    "        print(\"\\nâš ï¸ No completed rows found in '{NEXT_CSV_PATH}'.\")\n",
    "        print(\"Have marked all recommended molecules as processed; they will not be recommended again.\")\n",
    "        print(f\"Processed/failed molecules: {all_names_in_batch}\")\n",
    "\n",
    "    else:\n",
    "        successful_names = df_success['name'].tolist()\n",
    "        new_y = torch.tensor(-df_success[['dG', 'dG_ts']].values, **tkwargs)\n",
    "        new_x = torch.tensor(df_success[FEATURE_COLUMNS].values, **tkwargs)\n",
    "    \n",
    "        train_X = torch.cat([train_X, new_x], dim=0)\n",
    "        train_Y = torch.cat([train_Y, new_y], dim=0)\n",
    "        \n",
    "        print(\"\\nâœ… Successfully added new data points!\")\n",
    "        print(f\"Molecule list: {successful_names}\")\n",
    "        \n",
    "        failed_names = list(set(all_names_in_batch) - set(successful_names))\n",
    "        if failed_names:\n",
    "            print(f\"â„¹ï¸  {len(failed_names)} error or incomplete calculations have been recorded and will not be recommended again: {failed_names}\")\n",
    "\n",
    "    iteration_counter += 1\n",
    "\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Current training set size: {len(train_X)}\")\n",
    "    print(f\"Total number of molecules processed/evaluated: {len(evaluated_names)}\")\n",
    "    print(\"\\nðŸ‘‰ You can now go back and re-run ã€Cell 4ã€‘ to recommend the next batch of candidates.\")\n",
    "\n",
    "    print(\"\\n--- Plotting state after this iteration ---\")\n",
    "\n",
    "    new_y_to_plot = torch.tensor(-df_success[['dG', 'dG_ts']].values, **tkwargs) if not df_success.empty else None\n",
    "    plot_pareto_front(\n",
    "        train_Y=train_Y, \n",
    "        iteration_counter=iteration_counter,\n",
    "        initial_size=INITIAL_TRAIN_SIZE,\n",
    "        new_points=new_y_to_plot,\n",
    "        save_path=f\"{PLOTS_DIR}/pareto_front_iter_{iteration_counter - 1}.png\"\n",
    "    )\n",
    "    \n",
    "    new_x_to_plot = new_x if not df_success.empty else None\n",
    "    plot_feature_space(\n",
    "        X_pool=X_CANDIDATES_POOL,\n",
    "        train_X=train_X,\n",
    "        initial_size=INITIAL_TRAIN_SIZE,\n",
    "        new_X=new_x_to_plot, \n",
    "        save_path=f\"{PLOTS_DIR}/feature_space_iter_{iteration_counter - 1}.png\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
